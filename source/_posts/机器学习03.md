---
title: 机器学习-决策树
tags: machine learning
categories: 《机器学习》笔记
date: 2022-07-13 16:43:30
math: true
---



## 决策树

### 基本流程

一般的，一棵决策树包含一个根结点、若干个内部结点和若干个叶结点；叶结点对应于决策结果，其他每个结点则对应于一个属性测试；每个结点包含的样本集合根据属性测试的结果被划分到子结点中；根结点包含样本全集。从根结点到每个叶结点的路径对应了一个判定测试序列。决策树学习的目的是为了产生一棵泛化能力强，即处理未见示例能力强的决策树。其基本流程遵循简单且直观的“分而治之”(divide-and-conquer)策略，如图所示：

![image-20220714200801923](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220714200801923.png)

决策树的生成是一个递归过程

### 划分选择

那么应该如何选择最优划分属性呢？目标是使得结点的纯度越来越高

#### 信息增益

![image-20220714201654008](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220714201654008.png)



![image-20220714201823523](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220714201823523.png)

#### 增益率

![image-20220714205227283](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220714205227283.png)

![image-20220714205457545](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220714205457545.png)

![image-20220714205433068](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220714205433068.png)

#### 基尼指数

![image-20220714205925313](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220714205925313.png)

### 剪枝处理

剪枝可以降低过拟合，其基本策略有：

- 预剪枝 
- 后剪枝

预剪枝是指在决策树生成过程中，对每个结点在划分前先进行估计，若当前结点的划分不能带来决策树泛化性能提升，则停止划分并将当前结点标记为叶结点；后剪枝则是先从训练集生成一棵完整的决策树，然后自底向上地对非叶结点进行考察，若将该结点对应的子树替换为叶结点能带来决策树泛化性能提升，则将该子树替换为叶结点

预剪枝使得决策树的很多分支都没有“展开”，这不仅降低了过拟合的风险，还显著减少了决策树的训练时间开销和测试时间开销。但另一方面，有些分支的当前划分虽不能提升泛化性能、甚至可能导致泛化性能暂时下降，但在其基础上进行的后续划分却有可能导致性能显著提高；预剪枝基于“贪心”本质禁止这些分支展开，给预剪枝决策树带来了欠拟合的风险

后剪枝决策树通常比预剪枝决策树保留了更多的分支。一般情形下，后剪枝决策树的欠拟合风险很小，泛化性能往往优于预剪枝决策树。但后剪枝过程是在生成完全决策树之后进行的，并且要自底向上地对树中的所有非叶结点进行逐一考察，因此其训练时间开销比未剪枝决策树和预剪枝决策树都要大得多

### 连续与缺失值

#### 连续值处理

二分法处理连续属性(连续属性离散化)，在C4.5决策树算法中使用

![image-20220716180226651](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220716180226651.png)

![image-20220716180247574](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220716180247574.png)

#### 缺失值处理

![image-20220716180520040](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220716180520040.png)

![image-20220716180652878](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220716180652878.png)

### 多变量决策树

单变量决策树的分类边界的每一段都是与坐标轴平行的，多变量决策树则可以实现斜划分，在此类决策树中，非叶结点不再是仅对某个属性，而是对属性的线性组合进行测试

换言之，每个非叶结点是一个形如$\sum_{i=1}^{d}{w_{i}a_{i}}=t$的线性分类器，其中$w_{i}$是属性$a_{i}$的权重，$w_{i}$和$t$可在该结点所含的样本集和属性集上学得，于是，与传统的“单变量决策树”(univariate decision tree)不同，在多变量决策树的学习过程中，不是为每个非叶结点寻找一个最优划分属性，而是试图建立一个合适的线性分类器