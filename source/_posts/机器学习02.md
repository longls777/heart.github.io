---
title: 机器学习-线性模型
tags: machine learning
categories: 《机器学习》笔记
date: 2022-07-11 16:53:30
math: true
---



## 线性模型

给定由$d$个属性描述的示例$x=(x_{1};x_{2};...;x_{d})$

线性模型试图学得一个通过属性的线性组合来进行预测的函数，即：
$$
f(x)=w_{1}x_{1}+w_{2}x_{2}+...+w_{d}x_{d}+b
$$
一般用向量形式写成
$$
f(x)=\mathbf{w^Tx}+b
$$
其中$\mathbf{w}=(w_{1};w_{2};...;w_{d})$

### 线性回归

首先只考虑只有一种输入属性，则：

![image-20220711170104747](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220711170104747.png)

如何确定最合适的$w$和$b$呢？可以使均方误差最小

![image-20220711170249245](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220711170249245.png)

这也称为最小二乘法，即试图找到一条直线，使所有样本到直线上的欧氏距离之和最小

求解$w$和$b$使$E_{(w,b)}=\sum_{i-1}^{m}{(y_{i}-wx_{i}-b)^2}$最小：

![image-20220711170754724](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220711170754724.png)

令上两式为零可得$w$和$b$最优解的闭式解：

![image-20220711171838351](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220711171838351.png)

![](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220711171846162.png)

更一般的形式是开头的$f(x)=\mathbf{w^Tx}+b$，使得$f(x_{i})\simeq y_{i}$，这称为多元线性回归

![image-20220711200834315](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220711200834315.png)

![image-20220711200921727](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220711200921727.png)

![image-20220711200934471](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220711200934471.png)

> 推导过程https://eli.thegreenplace.net/2014/derivation-of-the-normal-equation-for-linear-regression

当$X^TX$不是满秩矩阵时，需要引入正则化(regularization)项

### 广义线性模型

![image-20220713144210957](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713144210957.png)

其中$g(·)$称为联系函数

### 对数几率回归

对数几率函数，即sigmod函数：
$$
y=\frac{1}{1+e^{-z}}
$$
带入广义线性模型得到：

![image-20220713144355971](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713144355971.png)

即：

![image-20220713144406172](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713144406172.png)

将$y$视为样本$x$为正例的可能性，$1-y$视为其反例的可能性，则两者的比值$\frac{y}{1-y}$称为几率(odds)，即$x$为正例的相对可能性，取对数则为对数几率(log odds)

那么如何确定上式中的$w$和$b$呢？可以采用**极大似然法**

上式可重写为：

![image-20220713145905713](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713145905713.png)

则：

![image-20220713145915269](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713145915269.png)

![image-20220713150237839](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713150237839.png)

![image-20220713150758755](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713150758755.png)

![image-20220713150806729](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713150806729.png)

### 线性判别分析

Linear Discrimination Analysis，简称LDA，可用于二分类问题

LDA的思想非常朴素：给定训练样例集，设法将样例投影到一条直线上，使得同类样例的投影点尽可能接近、异类样例的投影点尽可能远离；在对新样本进行分类时，将其投影到同样的这条直线上，再根据投影点的位置来确定新样本的类别

![image-20220713153049081](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713153049081.png)

![image-20220713153241813](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713153241813.png)

![image-20220713153340728](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713153340728.png)

![image-20220713153442866](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713153442866.png)

​	![image-20220713153557781](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713153557781.png)

![image-20220713153706768](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713153706768.png)

![image-20220713153718194](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713153718194.png)

![image-20220713153730154](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713153730154.png)

![image-20220713153748406](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713153748406.png)

### 多分类学习

多分类学习的基本思路是“拆解法”，即将多分类任务拆为若干个二分类任务求解

具体来说，先对问题进行拆分，然后为拆出的每个二分类任务训练一个分类器；在测试时，对这些分类器的预测结果进行集成以获得最终的多分类结果。这里的关键是如何对多分类任务进行拆分，以及如何对多个分类器进行集成

最经典的拆分策略有三种：

- 一对一（One vs. One，简称OvO）
- 一对其余（One vs. Rest，简称OvR）
- 多对多（Many vs. Many，简称MvM）



假设有N个类别，OvO将这N个类别两两配对，产生N(N-1)/2个二分类任务，最终结果通过投票产生，即预测最多的类作为最终分类结果。OvR则是每次将一个类别作为正例，其他N-1个类别作为反例，训练N个分类器，若仅有一个分类器预测为正类，则对应的类别标记作为最终分类结果；若有多个分类器预测为正类,则通常考虑各分类器的预测置信度，选择置信度最大的类别标记作为分类结果

![image-20220713160043881](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713160043881.png)

容易看出，OvR只需训练N个分类器，而OvО需训练N(N - 1)/2个分类器，因此，OvO的存储开销和测试时间开销通常比OvR更大，但在训练时，OvR的每个分类器均使用全部训练样例，而OvO的每个分类器仅用到两个类的样例，因此，在类别很多时，OvO的训练时间开销通常比OvR更小，至于预测性能，则取决于具体的数据分布，在多数情形下两者差不多

MvM是每次将若干个类作为正类，若干个其他类作为反类。显然，OvO和OvR是MvM的特例。MvM的正、反类构造必须有特殊的设计，不能随意选取。一种最常用的MvM技术：“纠错输出码”(Error Correcting Output Codes，简称ECOC)

![image-20220713161831059](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713161831059.png)

![image-20220713161852654](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713161852654.png)

![image-20220713162034580](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713162034580.png)

![image-20220713162057232](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713162057232.png)

![image-20220713162106292](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713162106292.png)

### 类别不平衡问题

![image-20220713162359620](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713162359620.png)

![image-20220713162414670](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713162414670.png)

![image-20220713162435233](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713162435233.png)

![image-20220713162547300](http://longls777.oss-cn-beijing.aliyuncs.com/img/image-20220713162547300.png)